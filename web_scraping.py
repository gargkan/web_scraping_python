# -*- coding: utf-8 -*-
"""Web_Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ULW3B2T0X5xKyAEzqBObhdK2YU5rRoW
"""

import urllib.request
from urllib.request import urlopen, Request, HTTPError
from bs4 import BeautifulSoup
import csv
from datetime import datetime, timedelta

wiki = "https://www.thestar.com.my/search/?q=HIV&qsort=oldest&qrec=10&qstockcode=&pgno=1"

html = urlopen(wiki)

bs = BeautifulSoup(html,'lxml')
bs

base_url ="https://www.thestar.com.my/search/?q=HIV&qsort=oldest&qrec=10&qstockcode=&pgno="

url_list = ["{}{}".format(base_url, str(page)) for page in range(1,10)]
s = []
for url in url_list:
  print(url)
  s.append(url)
s



column_names = ["Title of the article", "Date"]
df = pd.DataFrame(columns = column_names)
for pg in s:
  #query the website and return the html to the variable 'page'
  page = urllib.request.urlopen(pg)
  try:
    search_response = urllib.request.urlopen(pg)
  except urllib.request.HTTPError:
    pass
  #parse the html using bs and store in soup
  soup = BeautifulSoup(page, 'html.parser')
  #Take div and its value  
  ls = [x.text.strip() for x in soup.find_all('h2', class_='f18')]
  
  ls1 = [x.text.strip() for x in soup.find_all('span', class_='timestamp')]
  
  df = df.append([{'Title of the article': x,'Date': y} for x,y in zip(ls, ls1)])
  
print(df)



